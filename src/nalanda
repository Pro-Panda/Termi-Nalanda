#! /usr/bin/python3

from bs4 import BeautifulSoup
import os
import requests
import json

join = os.path.join

INSTALL_PATH = join(os.path.expanduser("~"), ".nalanda-cli")
SLIDES_PATH = join(os.path.expanduser("~"), "BITS")
subs = json.load(open(join(INSTALL_PATH, "subjects.json")))
SUB_NAMES = list(subs.values())
SUB_URLS = list(subs.keys())
RANGE_SUBS = range(len(SUB_URLS))

ZIP_FILE_LINK = "http://nalanda.bits-pilani.ac.in/mod/folder/download_folder.php"
NALANDA_LOGIN = "http://nalanda.bits-pilani.ac.in/login/index.php"


def bold(text):
    return "\033[1m" + text + "\033[0m"


def get_news(session, news_urls):
    subject_news_url = [[] for x in RANGE_SUBS]
    for x in RANGE_SUBS:
        for y in range(len(news_urls[x])):
            result = session.get(news_urls[x][y])
            soup = BeautifulSoup(result.text, "html.parser")
            discussion_list = soup.find_all("tr", "discussion")
            for url in discussion_list:
                if url.find("td", "topic starter pinned"):
                    subject_news_url[x].append([url.contents[0].contents[1].get(
                        "href"), url.contents[0].contents[1].contents[0]])
                else:
                    subject_news_url[x].append([url.contents[0].contents[0].get(
                        "href"), url.contents[0].contents[0].contents[0]])
    return subject_news_url


def find_new(session, urls_title, news_urls):
    new_urls_title = [[]for x in RANGE_SUBS]
    for x in RANGE_SUBS:
        subject_file = open(os.path.join(INSTALL_PATH, SUB_NAMES[x]), "a+")
        subject_file.seek(0)
        subject_read = subject_file.read().split("\n")
        for y in (urls_title[x] + news_urls[x]):
            if y[0] not in subject_read:
                new_urls_title[x].append(y)
                subject_file.write("\n" + y[0])
    return new_urls_title


def display(update_news, new_lectures):
    print(bold("News:"))
    if(sum([len(x) for x in update_news])==0):
        print("\tNo updates")
    else:
        for x in RANGE_SUBS:
            if update_news[x] != []:
                print(bold("\n" + SUB_NAMES[x] + "-"))
            for y in range(len(update_news[x])):
                print("\t" + bold(str(y + 1)) + ". " + update_news[x][y][1] + "\n\t\t" + \
                    update_news[x][y][0])

    print ("-" * 60 + "\n")

    print(bold("Lectures:"))
    if not new_lectures:
        print("\tNo updates")
    else:
        for x in new_lectures:
            print ("\t" + bold(SUB_NAMES[x]) + " has new updates")
        print ("\tfile://" + SLIDES_PATH)


def download(session, res_urls):
    for sub in SUB_NAMES:
        sub_path = join(SLIDES_PATH, sub)
        if not os.path.exists(sub_path):
            os.makedirs(sub_path)

    updates_sub_index = []
    for x in RANGE_SUBS:
        done_slides_file = open(join(INSTALL_PATH, SUB_NAMES[x] + ".lec"), "a+")
        done_slides_file.seek(0)
        done_slides = done_slides_file.read().split("\n")
        new_slides = list(set(res_urls[x]) - set(done_slides))
        if new_slides:
            updates_sub_index.append(x)
        for y in new_slides:
            if "folder/view" in y:
                id_param = y.split("php")[1]
                result = session.get(ZIP_FILE_LINK + id_param)
            else:
                result = session.get(y)

            file_name = result.headers["content-disposition"].split('e="')[1].split('"')[0]
            with open(join(SLIDES_PATH, SUB_NAMES[x], file_name), "wb") as f:
                f.write(result.content)
            done_slides_file.write(y + "\n")
    
    return updates_sub_index


if __name__ == "__main__":
    try:
        print ("\t\t" + bold("**nalanda-cli**"))
        session = requests.session()
        session.post(NALANDA_LOGIN, data=json.load(open(join(INSTALL_PATH, "config.json"))))

        links = [BeautifulSoup(session.get(sub).text, "html.parser").find_all("a", {"onclick": ""}) for sub in SUB_URLS]
        res_urls, news_urls, notice_urls = ([[] for x in RANGE_SUBS] for y in range(3))

        for x in range(len(links)):
            for y in range(len(links[x])):
                url = (links[x][y]).get("href")
                if("resource/view.php?id" in url or "folder/view.php?id=" in url):
                    res_urls[x].append(url)
                elif("page/view.php?id" in url):
                    notice_urls[x].append(
                        [url, links[x][y].contents[1].contents[0]])
                elif("forum/view.php?id" in url):
                    news_urls[x].append(url)

        subject_news_url = get_news(session, news_urls)
        unread_news = find_new(session, notice_urls, subject_news_url)
        new_lectures = download(session, res_urls)
        display(unread_news, new_lectures)

    except requests.exceptions.ConnectionError:
        quit("No Internet Connection. Please retry")
    except IOError:
        quit("Unable to read from file. Please reinstall termi-Nalanda")
    except KeyboardInterrupt:
        print("Stopped by user.")
